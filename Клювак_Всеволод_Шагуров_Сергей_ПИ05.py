# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ph1p6JCnfcWOppl-SDLYPiTKEtN37vRG
"""

!pip install -q kaggle

import os, json, shutil

# 1) –ø–µ—Ä–µ–º–µ—â–∞–µ–º kaggle.json, –∫–æ—Ç–æ—Ä—ã–π —Ç—ã –∑–∞–≥—Ä—É–∑–∏—à—å —á–µ—Ä–µ–∑ Files –≤ /content
os.makedirs(os.path.join(os.path.expanduser("~"), ".kaggle"), exist_ok=True)
if os.path.exists("kaggle.json"):
    shutil.move("kaggle.json", os.path.join(os.path.expanduser("~"), ".kaggle", "kaggle.json"))
    print("kaggle.json –ø–µ—Ä–µ–º–µ—â—ë–Ω –≤ ~/.kaggle")
else:
    print("‚ö†Ô∏è –ó–∞–≥—Ä—É–∑–∏—Ç–µ kaggle.json –≤ /content —á–µ—Ä–µ–∑ –ø–∞–Ω–µ–ª—å Files!")

os.chmod(os.path.join(os.path.expanduser("~"), ".kaggle", "kaggle.json"), 0o600)

# 2) —Å–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É data –∏ —Å–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç Fantacher "Metal Surface Defects Dataset" [web:25][web:28]
os.makedirs("data", exist_ok=True)

!kaggle datasets download -d fantacher/neu-metal-surface-defects-data -p data

# 3) —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º zip
import zipfile
zip_path = "data/neu-metal-surface-defects-data.zip"
with zipfile.ZipFile(zip_path, "r") as zf:
    zf.extractall("data")
print("–†–∞—Å–ø–∞–∫–æ–≤–∞–Ω–æ –≤ ./data")

import tensorflow as tf

# –∏—â–µ–º –ø–∞–ø–∫—É, –≥–¥–µ –ª–µ–∂–∞—Ç 6 –ø–æ–¥–ø–∞–ø–æ–∫ –∫–ª–∞—Å—Å–æ–≤ (Crazing, Inclusion, Patches, Pitted, Rolled, Scratches) [web:25][web:33]
def find_class_root(base_dir):
    for root, dirs, files in os.walk(base_dir):
        lower_dirs = [d.lower() for d in dirs]
        needed = ["crazing", "inclusion", "patches", "pitted", "rolled", "scratches"]
        if all(any(nd == need for nd in lower_dirs) for need in needed):
            return root
    return None

BASE_DIR = "./data"
train_root = find_class_root(BASE_DIR)
print("train_root =", train_root)
print("–ü–æ–¥–ø–∞–ø–∫–∏:", os.listdir(train_root))

CONFIG = {
    "img_height": 128,
    "img_width": 128,
    "batch_size": 32,
    "seed": 42,
    "val_split": 0.2,
    "test_split": 0.1,
    "epochs": 15,
    "base_filters": 32,
    "conv_blocks": 3,
    "dropout": 0.4,
    "learning_rate": 1e-3,
}

IMG_SIZE   = (CONFIG["img_height"], CONFIG["img_width"])
BATCH_SIZE = CONFIG["batch_size"]
SEED       = CONFIG["seed"]

full_ds = tf.keras.utils.image_dataset_from_directory(
    train_root,
    labels="inferred",
    label_mode="int",
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    seed=SEED,
    shuffle=True,
)

class_names = full_ds.class_names
num_classes = len(class_names)
print("class_names:", class_names, "num_classes =", num_classes)

total_batches = tf.data.experimental.cardinality(full_ds).numpy()
val_batches   = int(total_batches * CONFIG["val_split"])
test_batches  = int(total_batches * CONFIG["test_split"])
train_batches = total_batches - val_batches - test_batches

print(f"–í—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {total_batches}")
print(f"Train: {train_batches}, Val: {val_batches}, Test: {test_batches}")

train_ds = full_ds.take(train_batches)
val_ds   = full_ds.skip(train_batches).take(val_batches)
test_ds  = full_ds.skip(train_batches + val_batches)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)
val_ds   = val_ds.cache().prefetch(AUTOTUNE)
test_ds  = test_ds.cache().prefetch(AUTOTUNE)

train_root = "./data/NEU Metal Surface Defects Data/train"
print("train_root =", train_root)
print("–ü–æ–¥–ø–∞–ø–∫–∏:", os.listdir(train_root))

IMG_SIZE   = (CONFIG["img_height"], CONFIG["img_width"])
BATCH_SIZE = CONFIG["batch_size"]
SEED       = CONFIG["seed"]

full_ds = tf.keras.utils.image_dataset_from_directory(
    train_root,
    labels="inferred",
    label_mode="int",
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    seed=SEED,
    shuffle=True,
)

class_names = full_ds.class_names
num_classes = len(class_names)

total_batches = tf.data.experimental.cardinality(full_ds).numpy()
val_batches   = int(total_batches * CONFIG["val_split"])
test_batches  = int(total_batches * CONFIG["test_split"])
train_batches = total_batches - val_batches - test_batches

print(f"–í—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {total_batches}")
print(f"Train: {train_batches}, Val: {val_batches}, Test: {test_batches}")

import os, tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# 1. –î–ê–ù–ù–´–ï
train_root = "./data/NEU Metal Surface Defects Data/train"
print("train_root =", train_root)
print("–ü–æ–¥–ø–∞–ø–∫–∏:", os.listdir(train_root))

IMG_SIZE   = (CONFIG["img_height"], CONFIG["img_width"])
BATCH_SIZE = CONFIG["batch_size"]
SEED       = CONFIG["seed"]

full_ds = tf.keras.utils.image_dataset_from_directory(
    train_root,
    labels="inferred",
    label_mode="int",
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    seed=SEED,
    shuffle=True,
)

class_names = full_ds.class_names
num_classes = len(class_names)
print("class_names:", class_names, "num_classes =", num_classes)

total_batches = tf.data.experimental.cardinality(full_ds).numpy()
val_batches   = int(total_batches * CONFIG["val_split"])
test_batches  = int(total_batches * CONFIG["test_split"])
train_batches = total_batches - val_batches - test_batches

print(f"–í—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {total_batches}")
print(f"Train: {train_batches}, Val: {val_batches}, Test: {test_batches}")

train_ds = full_ds.take(train_batches)
val_ds   = full_ds.skip(train_batches).take(val_batches)
test_ds  = full_ds.skip(train_batches + val_batches)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)
val_ds   = val_ds.cache().prefetch(AUTOTUNE)
test_ds  = test_ds.cache().prefetch(AUTOTUNE)

print("train_ds batches:", tf.data.experimental.cardinality(train_ds).numpy())
print("val_ds batches:",   tf.data.experimental.cardinality(val_ds).numpy())
print("test_ds batches:",  tf.data.experimental.cardinality(test_ds).numpy())

# 2. –ú–û–î–ï–õ–¨
data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.1),
    ]
)

def build_cnn_model(
    img_size=IMG_SIZE,
    num_classes=num_classes,
    base_filters=32,
    conv_blocks=3,
    dropout=0.4,
):
    inputs = keras.Input(shape=img_size + (3,))
    x = data_augmentation(inputs)
    x = layers.Rescaling(1.0 / 255)(x)

    filters = base_filters
    for _ in range(conv_blocks):
        x = layers.Conv2D(filters, 3, padding="same", activation="relu")(x)
        x = layers.Conv2D(filters, 3, padding="same", activation="relu")(x)
        x = layers.MaxPooling2D()(x)
        filters *= 2

    x = layers.Flatten()(x)
    x = layers.Dense(256, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    model = keras.Model(inputs, outputs, name="metal_defect_cnn")
    return model

model = build_cnn_model(
    img_size=IMG_SIZE,
    num_classes=num_classes,
    base_filters=CONFIG["base_filters"],
    conv_blocks=CONFIG["conv_blocks"],
    dropout=CONFIG["dropout"],
)

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=CONFIG["learning_rate"]),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)

model.summary()

callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=5,
        restore_best_weights=True,
    )
]

# 3. –û–ë–£–ß–ï–ù–ò–ï
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=CONFIG["epochs"],
    callbacks=callbacks,
)

test_loss, test_acc = model.evaluate(test_ds)
print(f"Test accuracy: {test_acc:.4f}, loss: {test_loss:.4f}")

# 4. –ì–†–ê–§–ò–ö–ò
acc      = history.history["accuracy"]
val_acc  = history.history["val_accuracy"]
loss     = history.history["loss"]
val_loss = history.history["val_loss"]
epochs_range = range(1, len(acc) + 1)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label="–û–±—É—á–∞—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
plt.plot(epochs_range, val_acc, label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
plt.title("–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label="–û–±—É—á–∞—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.plot(epochs_range, val_loss, label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.title("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
plt.legend()

plt.tight_layout()
plt.show()

def build_cnn_model_v2(img_size=IMG_SIZE, num_classes=num_classes):
    inputs = keras.Input(shape=img_size + (3,))
    x = data_augmentation(inputs)
    x = layers.Rescaling(1.0 / 255)(x)

    # –ø—Ä–∏–º–µ—Ä: —Ç–æ–ª—å–∫–æ 2 –±–ª–æ–∫–∞ –≤–º–µ—Å—Ç–æ 3
    for filters in [32, 64]:
        x = layers.Conv2D(filters, 3, padding="same", activation="relu")(x)
        x = layers.Conv2D(filters, 3, padding="same", activation="relu")(x)
        x = layers.MaxPooling2D()(x)

    x = layers.Flatten()(x)
    x = layers.Dense(128, activation="relu")(x)
    x = layers.Dropout(0.4)(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    return keras.Model(inputs, outputs, name="metal_defect_cnn_v2")

model2 = build_cnn_model_v2()
model2.compile(
    optimizer=keras.optimizers.Adam(learning_rate=CONFIG["learning_rate"]),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)

history2 = model2.fit(
    train_ds,
    validation_data=val_ds,
    epochs=CONFIG["epochs"],
)

test_loss2, test_acc2 = model2.evaluate(test_ds)
print("Model2 test acc:", test_acc2)

!pip install -q kaggle

import os, shutil, zipfile

# 1) kaggle.json –∑–∞–≥—Ä—É–∑–∏ —á–µ—Ä–µ–∑ –ø–∞–Ω–µ–ª—å Files –≤ /content, –ø–æ—Ç–æ–º:
os.makedirs(os.path.join(os.path.expanduser("~"), ".kaggle"), exist_ok=True)
if os.path.exists("kaggle.json"):
    shutil.move("kaggle.json", os.path.join(os.path.expanduser("~"), ".kaggle", "kaggle.json"))
    print("kaggle.json –ø–µ—Ä–µ–º–µ—â—ë–Ω")
os.chmod(os.path.join(os.path.expanduser("~"), ".kaggle", "kaggle.json"), 0o600)

# 2) —Å–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
os.makedirs("data", exist_ok=True)
!kaggle datasets download -d fantacher/neu-metal-surface-defects-data -p data

import zipfile, os

zip_path = "data/neu-metal-surface-defects-data.zip"
extract_dir = "data"

with zipfile.ZipFile(zip_path, "r") as zf:
    zf.extractall(extract_dir)

print("–°–æ–¥–µ—Ä–∂–∏–º–æ–µ ./data:", os.listdir("data"))

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import os

CONFIG = {
    "img_height": 128,
    "img_width": 128,
    "batch_size": 32,
    "seed": 42,
    "val_split": 0.2,
    "test_split": 0.1,
    "epochs": 10,
    "learning_rate": 1e-3,
}

train_root = "./data/NEU Metal Surface Defects Data/train"
print("train_root =", train_root)
print("–ü–æ–¥–ø–∞–ø–∫–∏ train:", os.listdir(train_root))

IMG_SIZE   = (CONFIG["img_height"], CONFIG["img_width"])
BATCH_SIZE = CONFIG["batch_size"]
SEED       = CONFIG["seed"]

full_ds = tf.keras.utils.image_dataset_from_directory(
    train_root,
    labels="inferred",
    label_mode="int",
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    seed=SEED,
    shuffle=True,
)

class_names = full_ds.class_names
num_classes = len(class_names)
print("class_names:", class_names, "num_classes =", num_classes)

total_batches = tf.data.experimental.cardinality(full_ds).numpy()
val_batches   = int(total_batches * CONFIG["val_split"])
test_batches  = int(total_batches * CONFIG["test_split"])
train_batches = total_batches - val_batches - test_batches

print(f"–í—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {total_batches}")
print(f"Train: {train_batches}, Val: {val_batches}, Test: {test_batches}")

train_ds = full_ds.take(train_batches)
val_ds   = full_ds.skip(train_batches).take(val_batches)
test_ds  = full_ds.skip(train_batches + val_batches)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)
val_ds   = val_ds.cache().prefetch(AUTOTUNE)
test_ds  = test_ds.cache().prefetch(AUTOTUNE)

data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.1),
    ]
)

def build_cnn_model_v2(img_size=IMG_SIZE, num_classes=num_classes):
    inputs = keras.Input(shape=img_size + (3,))
    x = data_augmentation(inputs)
    x = layers.Rescaling(1.0 / 255)(x)

    for filters in [32, 64]:
        x = layers.Conv2D(filters, 3, padding="same", activation="relu")(x)
        x = layers.Conv2D(filters, 3, padding="same", activation="relu")(x)
        x = layers.MaxPooling2D()(x)

    x = layers.Flatten()(x)
    x = layers.Dense(128, activation="relu")(x)
    x = layers.Dropout(0.4)(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    return keras.Model(inputs, outputs, name="metal_defect_cnn_v2")

model2 = build_cnn_model_v2()
model2.compile(
    optimizer=keras.optimizers.Adam(learning_rate=CONFIG["learning_rate"]),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)

history2 = model2.fit(
    train_ds,
    validation_data=val_ds,
    epochs=CONFIG["epochs"],
)

test_loss2, test_acc2 = model2.evaluate(test_ds)
print(f"Model2 test accuracy: {test_acc2:.4f}, loss: {test_loss2:.4f}")

# –ì—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –ê–ª–≥–æ—Ä–∏—Ç–º–∞ 2
acc      = history2.history["accuracy"]
val_acc  = history2.history["val_accuracy"]
loss     = history2.history["loss"]
val_loss = history2.history["val_loss"]
epochs   = range(1, len(acc) + 1)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label="–û–±—É—á–∞—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
plt.plot(epochs, val_acc, label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
plt.title("–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ê–ª–≥–æ—Ä–∏—Ç–º 2)")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label="–û–±—É—á–∞—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.plot(epochs, val_loss, label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.title("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ê–ª–≥–æ—Ä–∏—Ç–º 2)")
plt.legend()

plt.tight_layout()
plt.show()

from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# –ú–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –≤–∑—è—Ç—å —Ç–∞–∫—É—é –∂–µ, –∫–∞–∫ –≤ –ê–ª–≥–æ—Ä–∏—Ç–º–µ 2
def build_cnn_model_v2(img_size=IMG_SIZE, num_classes=num_classes):
    inputs = keras.Input(shape=img_size + (3,))
    x = data_augmentation(inputs)
    x = layers.Rescaling(1.0 / 255)(x)

    for filters in [32, 64]:
        x = layers.Conv2D(filters, 3, padding="same", activation="relu")(x)
        x = layers.Conv2D(filters, 3, padding="same", activation="relu")(x)
        x = layers.MaxPooling2D()(x)

    x = layers.Flatten()(x)
    x = layers.Dense(128, activation="relu")(x)
    x = layers.Dropout(0.4)(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    return keras.Model(inputs, outputs, name="metal_defect_cnn_v2_sgd")

model3 = build_cnn_model_v2()

# –í–ê–ñ–ù–û: –¥—Ä—É–≥–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä ‚Äî SGD —Å momentum
opt = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)  # –∞–ª–≥–æ—Ä–∏—Ç–º 3

model3.compile(
    optimizer=opt,
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)

model3.summary()

history3 = model3.fit(
    train_ds,
    validation_data=val_ds,
    epochs=CONFIG["epochs"],
)

test_loss3, test_acc3 = model3.evaluate(test_ds)
print(f"Model3 (SGD) test accuracy: {test_acc3:.4f}, loss: {test_loss3:.4f}")

# –ì—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –ê–ª–≥–æ—Ä–∏—Ç–º–∞ 3
acc      = history3.history["accuracy"]
val_acc  = history3.history["val_accuracy"]
loss     = history3.history["loss"]
val_loss = history3.history["val_loss"]
epochs   = range(1, len(acc) + 1)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label="–û–±—É—á–∞—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
plt.plot(epochs, val_acc, label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
plt.title("–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ê–ª–≥–æ—Ä–∏—Ç–º 3, SGD)")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label="–û–±—É—á–∞—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.plot(epochs, val_loss, label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.title("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ê–ª–≥–æ—Ä–∏—Ç–º 3, SGD)")
plt.legend()

plt.tight_layout()
plt.show()

!pip install -q kaggle

import os, shutil, zipfile

# 1) –ó–∞–≥—Ä—É–∑–∏—Ç–µ kaggle.json —á–µ—Ä–µ–∑ –ø–∞–Ω–µ–ª—å Files –≤ /content
os.makedirs(os.path.join(os.path.expanduser("~"), ".kaggle"), exist_ok=True)
if os.path.exists("kaggle.json"):
    shutil.move("kaggle.json", os.path.join(os.path.expanduser("~"), ".kaggle", "kaggle.json"))
    print("kaggle.json –ø–µ—Ä–µ–º–µ—â—ë–Ω –≤ ~/.kaggle")
else:
    print("‚ö†Ô∏è –ó–∞–≥—Ä—É–∑–∏—Ç–µ kaggle.json –≤ –∫–æ—Ä–µ–Ω—å /content!")

os.chmod(os.path.join(os.path.expanduser("~"), ".kaggle", "kaggle.json"), 0o600)

# 2) –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç Fantacher NEU Metal Surface Defects [web:25][web:163]
os.makedirs("data", exist_ok=True)
!kaggle datasets download -d fantacher/neu-metal-surface-defects-data -p data

import zipfile, os

zip_path = "data/neu-metal-surface-defects-data.zip"
extract_dir = "data"

with zipfile.ZipFile(zip_path, "r") as zf:
    zf.extractall(extract_dir)

print("–°–æ–¥–µ—Ä–∂–∏–º–æ–µ ./data:", os.listdir("data"))

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import os

CONFIG = {
    "img_height": 128,
    "img_width": 128,
    "batch_size": 32,
    "seed": 42,
    "val_split": 0.2,
    "test_split": 0.1,
    "epochs": 10,
    "learning_rate": 1e-3,
}

train_root = "./data/NEU Metal Surface Defects Data/train"
print("train_root =", train_root)
print("–ü–æ–¥–ø–∞–ø–∫–∏ train:", os.listdir(train_root))

IMG_SIZE   = (CONFIG["img_height"], CONFIG["img_width"])
BATCH_SIZE = CONFIG["batch_size"]
SEED       = CONFIG["seed"]

full_ds = tf.keras.utils.image_dataset_from_directory(
    train_root,
    labels="inferred",
    label_mode="int",
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    seed=SEED,
    shuffle=True,
)

class_names = full_ds.class_names
num_classes = len(class_names)
print("class_names:", class_names, "num_classes =", num_classes)

total_batches = tf.data.experimental.cardinality(full_ds).numpy()
val_batches   = int(total_batches * CONFIG["val_split"])
test_batches  = int(total_batches * CONFIG["test_split"])
train_batches = total_batches - val_batches - test_batches

print(f"–í—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {total_batches}")
print(f"Train: {train_batches}, Val: {val_batches}, Test: {test_batches}")

train_ds = full_ds.take(train_batches)
val_ds   = full_ds.skip(train_batches).take(val_batches)
test_ds  = full_ds.skip(train_batches + val_batches)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)
val_ds   = val_ds.cache().prefetch(AUTOTUNE)
test_ds  = test_ds.cache().prefetch(AUTOTUNE)

data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.1),
    ]
)

import matplotlib.pyplot as plt
from tensorflow.keras import applications

# 1) –ë–∞–∑–æ–≤–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å MobileNetV2 [web:111]
base_model = applications.MobileNetV2(
    input_shape=IMG_SIZE + (3,),
    include_top=False,
    weights="imagenet"
)
base_model.trainable = False  # —Å–Ω–∞—á–∞–ª–∞ –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã –≤—Å–µ —Å–ª–æ–∏

# 2) –°—Ç—Ä–æ–∏–º —Å–≤–æ—é ¬´–≥–æ–ª–æ–≤—É¬ª –¥–ª—è 6 –∫–ª–∞—Å—Å–æ–≤ –¥–µ—Ñ–µ–∫—Ç–æ–≤
inputs = keras.Input(shape=IMG_SIZE + (3,))
x = data_augmentation(inputs)
x = applications.mobilenet_v2.preprocess_input(x)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(128, activation="relu")(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)

model4 = keras.Model(inputs, outputs, name="mobilenetv2_defects")

model4.compile(
    optimizer=keras.optimizers.Adam(learning_rate=CONFIG["learning_rate"]),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)

model4.summary()

# 3) –û–±—É—á–µ–Ω–∏–µ (–ê–ª–≥–æ—Ä–∏—Ç–º 4)
history4 = model4.fit(
    train_ds,
    validation_data=val_ds,
    epochs=CONFIG["epochs"],
)

test_loss4, test_acc4 = model4.evaluate(test_ds)
print(f"MobileNetV2 test accuracy: {test_acc4:.4f}, loss: {test_loss4:.4f}")

acc      = history4.history["accuracy"]
val_acc  = history4.history["val_accuracy"]
loss     = history4.history["loss"]
val_loss = history4.history["val_loss"]
epochs   = range(1, len(acc) + 1)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label="–û–±—É—á–∞—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
plt.plot(epochs, val_acc, label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
plt.title("–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ê–ª–≥–æ—Ä–∏—Ç–º 4, MobileNetV2)")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label="–û–±—É—á–∞—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.plot(epochs, val_loss, label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.title("–ü–æ—Ç–µ—Ä–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ê–ª–≥–æ—Ä–∏—Ç–º 4, MobileNetV2)")
plt.legend()

plt.tight_layout()
plt.show()

import gc
import tensorflow as tf
import numpy as np

# 1) –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU/TPU
tf.keras.backend.clear_session()
gc.collect()

# 2) –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
try:
    del model4, model5, base_model
    del train_ds, val_ds, test_ds, full_ds
    del history4, history_fine, history_combined
except:
    pass

# 3) –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è TF 2.x (set_learning_phase —É—Å—Ç–∞—Ä–µ–ª)
tf.config.optimizer.set_jit(False)  # XLA off
gc.collect()

# 4) –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –ø–∞–º—è—Ç–∏
try:
    gpu_info = tf.config.experimental.get_memory_info('GPU:0')
    print(f"‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞. GPU: {gpu_info['current'] / 1e9:.1f} GB")
except:
    print("‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞ (CPU mode)")

print("üöÄ –ì–æ—Ç–æ–≤ –∫ –ê–ª–≥–æ—Ä–∏—Ç–º—É 5!")

import tensorflow as tf
from tensorflow.keras import layers, applications
import matplotlib.pyplot as plt

print("=== –ê–õ–ì–û–†–ò–¢–ú 5: Fine-tuning MobileNetV2 (–ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞) ===")

# 1) –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ MobileNetV2 —Å –Ω—É–ª—è
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
train_root = "./data/NEU Metal Surface Defects Data/train"

full_ds = tf.keras.utils.image_dataset_from_directory(
    train_root, image_size=IMG_SIZE, batch_size=BATCH_SIZE, seed=42, shuffle=True
)
class_names = full_ds.class_names
num_classes = len(class_names)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
total = tf.data.experimental.cardinality(full_ds).numpy()
train_ds = full_ds.take(30).cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)
val_ds = full_ds.skip(30).take(4).cache().prefetch(tf.data.AUTOTUNE)
test_ds = full_ds.skip(34)

# 2) –°–æ–∑–¥–∞—ë–º MobileNetV2 –∑–∞–Ω–æ–≤–æ (–∫–∞–∫ –≤ –ê–ª–≥–æ—Ä–∏—Ç–º–µ 4)
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])

base_model = applications.MobileNetV2(
    input_shape=IMG_SIZE + (3,), include_top=False, weights="imagenet"
)
base_model.trainable = False

inputs = tf.keras.Input(shape=IMG_SIZE + (3,))
x = data_augmentation(inputs)
x = applications.mobilenet_v2.preprocess_input(x)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(128, activation="relu")(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)

model5 = tf.keras.Model(inputs, outputs)
model5.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (10 —ç–ø–æ—Ö)
print("üîÑ –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π MobileNetV2...")
history_base = model5.fit(train_ds, validation_data=val_ds, epochs=10, verbose=1)

test_loss_base, test_acc_base = model5.evaluate(test_ds, verbose=0)
print(f"–ë–∞–∑–æ–≤–∞—è MobileNetV2: test acc = {test_acc_base:.4f}")

# 3) Fine-tuning: —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Å–ª–æ—ë–≤
base_model.trainable = True
fine_tune_at = len(base_model.layers) - 20
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

model5.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),  # –ú–∞–ª–µ–Ω—å–∫–∏–π LR
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

print(f"üîÑ Fine-tuning –ø–æ—Å–ª–µ–¥–Ω–∏—Ö {len(base_model.layers) - fine_tune_at} —Å–ª–æ—ë–≤...")
history_fine = model5.fit(
    train_ds, validation_data=val_ds, epochs=5, verbose=1
)

test_loss5, test_acc5 = model5.evaluate(test_ds, verbose=0)
print(f"\n‚úÖ Fine-tune —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {test_acc5:.4f}")
print(f"–£–ª—É—á—à–µ–Ω–∏–µ: {test_acc5 - test_acc_base:+.4f}")

import matplotlib.pyplot as plt

# –ì—Ä–∞—Ñ–∏–∫ –¢–û–õ–¨–ö–û –¥–ª—è –ê–ª–≥–æ—Ä–∏—Ç–º–∞ 5 (–∫–∞–∫ —É –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ 1-4)
plt.figure(figsize=(12, 4))

# –¢–æ—á–Ω–æ—Å—Ç—å
plt.subplot(1, 2, 1)
plt.plot(history_fine.history["accuracy"], label="–û–±—É—á–∞—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
plt.plot(history_fine.history["val_accuracy"], label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
plt.title("–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ê–ª–≥–æ—Ä–∏—Ç–º 5, MobileNetV2 Fine-tune)")
plt.legend()
plt.grid(True)

# –ü–æ—Ç–µ—Ä–∏
plt.subplot(1, 2, 2)
plt.plot(history_fine.history["loss"], label="–û–±—É—á–∞—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.plot(history_fine.history["val_loss"], label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.xlabel("–≠–ø–æ—Ö–∞")
plt.ylabel("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
plt.title("–ü–æ—Ç–µ—Ä–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ê–ª–≥–æ—Ä–∏—Ç–º 5, MobileNetV2 Fine-tune)")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

print(f"‚úÖ –ê–ª–≥–æ—Ä–∏—Ç–º 5: test accuracy = {test_acc5:.4f}")

